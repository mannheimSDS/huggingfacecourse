{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cd C:\\Users\\LeoWe\\Desktop\\Doktor\\LLM Papers\\LLM workshop\\.env\\Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9fc02bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LeoWe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# Make sure to download the necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37cb6452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Die Personen:\\nEin Ehepaar: Mann (M) und Frau (F)\\n\\nF: \"Wieso geht der Fernseher denn gerade heute kaputt?\"', 'M: \"Die bauen die Geräte absichtlich so, dass sie schnell kaputt gehen.\"', 'F: \"Ich muss nicht unbedingt Fernsehen.\"', 'M: \"Ich auch nicht.', 'Nicht nur, weil heute der Apparat kaputt ist, ich meine sowieso, ich sehe sowieso nicht gerne Fernsehen.\"', 'F: \"Es ist ja auch wirklich NICHTS im Fernsehen, was man gern sehen möchte.\"', 'M: \"Heute brauchen wir, Gott sein Dank, überhaupt nicht erst in den blöden Kasten zu gucken.\"', 'F: \"Nee, es sieht aber so aus, als ob du hinguckst.\"', 'M: \"Ich?\"', 'F: \"Ja.\"', 'M: \"Nein, ich sehe nur ganz allgemein in diese Richtung.', 'Aber du guckst hin.', 'Du guckst da immer hin.\"', 'F: \"Ich?', 'Ich gucke dahin?', 'Wie kommst du denn darauf?\"', 'M: \"Es sieht so aus.\"', 'F: \"Das kann gar nicht so aussehen, ich gucke nämlich vorbei.', 'Ich gucke absichtlich vorbei.', 'Und wenn du ein kleines bisschen mehr auf mich achten würdest, hättest du bemerkt, dass ich absichtlich vorbei gucke.', 'Aber du interessierst dich ja überhaupt nicht für mich.\"', 'M: \"Jajajaja.\"', 'F: \"Wir können doch einfach mal ganz woanders hingucken.\"', 'M: \"Woanders?', 'Wohin denn?\"', 'F: \"Zur Seite, oder nach hinten.', 'M: \"Nach hinten?', 'Ich soll nach hinten sehen?', 'Nur weil der Fernseher kaputt ist, soll ich nach hinten sehen?', 'Ich lass mir doch von einem Fernsehgerät nicht vorschreiben, wo ich hinsehen soll.\"', 'F: \"Was wäre denn heute für ein Programm gewesen?\"', 'M: \"Eine Unterhaltungssendung.\"', 'F: \"Ach.\"', 'M: \"Es ist schon eine Unverschämtheit, was einem so Abend für Abend im Fernsehen geboten wird.', 'Ich weiß gar nicht, warum man sich das überhaupt noch ansieht.', 'Lesen könnte man statt dessen, Karten spielen oder ins Kino gehen oder ins Theater.', 'Statt dessen sitzt man da und glotzt auf dieses blöde Fernsehprogramm.\"', 'F: \"Heute ist der Apparat ja nun kaputt.\"', 'M: \"Gott sei Dank.\"', 'F: \"Ja.\"', 'M: \"Da kann man sich wenigstens mal unterhalten.\"', 'F: \"Oder früh ins Bett gehen.\"', 'M: \"Ich gehe nach den Spätnachrichten der Tagesschau ins Bett.\"', 'F: \"Aber der Fernseher ist doch kaputt.\"', 'M: \"Ich lasse mir von einem kaputten Fernseher nicht vorschreiben, wann ich ins Bett zu gehen habe.\"']\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your file\n",
    "file_path = 'C:/Users/LeoWe/Desktop/Doktor/LLM Papers/LLM workshop/Data/Loriot/fernsehabend.txt'  # Change this to the path of your file\n",
    "\n",
    "# Read the file contents\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    subtitle_text = file.read()\n",
    "\n",
    "# Read the file contents\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    subtitle_text = file.read()\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "batch_sentences = sent_tokenize(subtitle_text, language='german')\n",
    "\n",
    "print(batch_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a765a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenize_function(batch_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7291c69a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f92269-4a09-4d45-b386-469ef9e0498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\leowe\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.1.2)\n",
      "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
      "  Obtaining dependency information for accelerate>=0.20.3 from https://files.pythonhosted.org/packages/a6/b9/44623bdb05595481107153182e7f4b9f2ef9d3b674938ad13842054dcbd8/accelerate-0.26.1-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\leowe\\anaconda3\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
      "Using cached accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.26.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3e7039",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<string>:112\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1372\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[0;32m   1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1372\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1375\u001b[0m ):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m     )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1389\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1795\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_devices\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfget(obj)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1716\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available(min_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1716\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1717\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1718\u001b[0m         )\n\u001b[0;32m   1719\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
